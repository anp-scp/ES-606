{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from models import VGG_Style_5Block\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets.scalogramDataset import CustomBrainMNISTSCalogram\n",
    "min_loss_so_far = float('inf')\n",
    "\n",
    "modelPath = 'dlPipelineScalo/models/'\n",
    "tbPath = 'dlPipelineScalo/runs/'\n",
    "shutil.rmtree(modelPath, ignore_errors=True)\n",
    "shutil.rmtree(tbPath, ignore_errors=True)\n",
    "os.makedirs(modelPath)\n",
    "os.makedirs(tbPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64470,), (64470, 4, 30, 256))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorDir = 'processedData/channelSlectedBandPassed/npy/eventScalogram.npy'\n",
    "targetsDir = 'processedData/channelSlectedBandPassed/npy/targets.npy'\n",
    "npData = np.load(tensorDir)\n",
    "npTargets = np.load(targetsDir)\n",
    "randomIndex = np.random.permutation(npTargets.shape[0])\n",
    "npData = npData[randomIndex]\n",
    "npTargets = npTargets[randomIndex]\n",
    "npTargets.shape, npData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44470, 10000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainExtend = (0, 44470)\n",
    "validationExtend = (44470, 54470)\n",
    "testExtend = (54470, 64470)\n",
    "training_data = CustomBrainMNISTSCalogram(eventArrays=npData, targetsArrays=npTargets, extends=trainExtend)\n",
    "validation_data = CustomBrainMNISTSCalogram(eventArrays=npData, targetsArrays=npTargets, extends=validationExtend)\n",
    "testing_data = CustomBrainMNISTSCalogram(eventArrays=npData, targetsArrays=npTargets, extends=testExtend)\n",
    "len(training_data), len(validation_data), len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(training_args: dict):\n",
    "    global min_loss_so_far\n",
    "    batch_size = int(training_args.get('batch_size', 8))\n",
    "    device = training_args.get('dev_id')\n",
    "    lr=float(training_args.get('lr',0.0001))\n",
    "    epochs = int(training_args.get('num_epochs',10))\n",
    "    tbPath = training_args['tbPath']\n",
    "    modelPath = training_args['modelPath']\n",
    "    # modelParams = training_args['modelParams']\n",
    "    # print((batch_size, device, lr, epochs, tbPath, modelPath))\n",
    "    model_name = '_epochs=' + str(epochs) + '_lr=' + str(lr) + '_batchSize=' + str(batch_size)\n",
    "    tb = SummaryWriter(log_dir=tbPath + model_name)\n",
    "\n",
    "    training_dataloader = DataLoader(training_data, shuffle=True, batch_size=batch_size)\n",
    "    validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "    model = VGG_Style_5Block(in_channels=4)\n",
    "    print(model)\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model = model.to(device)\n",
    "    train_step = 0\n",
    "    val_step = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        ## training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_train_batches = len(training_dataloader)\n",
    "        for batch, (X, y) in enumerate(training_dataloader):\n",
    "            train_step += 1\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_criterion(pred, y)\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "            train_loss += loss\n",
    "            tb.add_scalar('training loss vs step', loss, train_step)\n",
    "        train_loss /= num_train_batches\n",
    "\n",
    "        #validating\n",
    "        val_size = len(validation_dataloader.dataset)\n",
    "        num_val_batches = len(validation_dataloader)\n",
    "        val_loss, correct = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in validation_dataloader:\n",
    "                val_step += 1\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = loss_criterion(pred, y).item()\n",
    "                val_loss += loss\n",
    "                tb.add_scalar('validation loss vs step', loss, val_step)\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        val_loss /= num_val_batches\n",
    "        correct /= val_size\n",
    "        accuracy = 100*correct\n",
    "        if val_loss < min_loss_so_far:\n",
    "            min_loss_so_far = val_loss\n",
    "            checkpoint = dict()\n",
    "            checkpoint['sd'] = model.state_dict()\n",
    "            checkpoint['h_params'] = model_name\n",
    "            checkpoint['saved_epoch'] = epoch\n",
    "            torch.save(checkpoint, modelPath + 'bestModelSoFar.pt')\n",
    "        tb.add_scalar('average training loss vs epoch', train_loss, epoch)\n",
    "        tb.add_scalar('average validation loss vs epoch', val_loss, epoch)\n",
    "        tb.add_scalar('validation accuracy vs epoch', accuracy, epoch)\n",
    "    \n",
    "    del model\n",
    "    del training_dataloader\n",
    "    del validation_dataloader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempModel = CNN001().to(torch.device(\"cuda:1\"))\n",
    "# summary(tempModel, (1,4,256))\n",
    "# del tempModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN002(*(\n",
    "#     (4,8,(30,1),1,0),\n",
    "#     (8,16,(1,64),1,0),\n",
    "#     (4,30,256)\n",
    "# ))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Run ID: 0 | H Params==> lr:0.001, batch_size:100, epochs:200\n",
      "VGG_Style_5Block(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Flatten(start_dim=1, end_dim=-1)\n",
      "    (16): LazyLinear(in_features=0, out_features=128, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharmaanupam/.conda/envs/anp_104/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd782f1a2bb471895aba4f653fed6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = dict(\n",
    "    lr = [0.001],\n",
    "    batch_size = [100],\n",
    "    epochs = [200]\n",
    ")\n",
    "\n",
    "# modelParams = (\n",
    "#     (4,8,(30,1),1,0),\n",
    "#     (8,16,(1,64),1,0),\n",
    "#     (4,30,256)\n",
    "# )\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "## performing hyper paramete tuning\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "for run_id, (lr,batch_size, epochs) in enumerate(product(*param_values)):\n",
    "    print(\"Run ID: {} | H Params==> lr:{}, batch_size:{}, epochs:{}\".format(run_id, lr, batch_size, epochs))\n",
    "    training_args = dict()\n",
    "    training_args['num_epochs'] = epochs\n",
    "    training_args['lr'] = lr\n",
    "    training_args['batch_size'] = batch_size\n",
    "    training_args['dev_id'] = device\n",
    "    training_args['tbPath'] = tbPath\n",
    "    training_args['modelPath'] = modelPath\n",
    "    # training_args['modelParams'] = modelParams\n",
    "    model_trainer(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper params:_epochs=200_lr=0.001_batchSize=100\n",
      "Validation loss when the model was saved:2.3023527383804323\n",
      "Epoch when the model was saved:126\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.302656 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(testing_data)\n",
    "## perfroming test\n",
    "checkpoint = torch.load(modelPath + 'bestModelSoFar.pt')\n",
    "model = VGG_Style_5Block(in_channels=4)\n",
    "model.load_state_dict(checkpoint['sd'])\n",
    "print(\"Model hyper params:{}\".format(checkpoint['h_params']))\n",
    "print(\"Validation loss when the model was saved:{}\".format(min_loss_so_far))\n",
    "print(\"Epoch when the model was saved:{}\".format(checkpoint['saved_epoch']))\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "test_size = len(test_loader.dataset)\n",
    "num_test_batches = len(test_loader)\n",
    "test_loss, correct = 0, 0\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_criteria(pred, y).item()\n",
    "        test_loss += loss\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "test_loss /= num_test_batches\n",
    "correct /= test_size\n",
    "accuracy = 100*correct\n",
    "print(f\"Test Error: \\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anp_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
