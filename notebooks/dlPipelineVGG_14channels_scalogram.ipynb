{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning pipeline for conducting the experiment with scalograms of EEG recording from 14 (all) channels  using the model architecture: VGG_Style_3Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from models import VGG_Style_3Block\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets.scalogramDataset import CustomBrainMNISTSCalogramAllChannel\n",
    "min_loss_so_far = float('inf')\n",
    "\n",
    "modelPath = 'dlPipelineScaloAllChannels/models/'\n",
    "tbPath = 'dlPipelineScaloAllChannels/runs/'\n",
    "shutil.rmtree(modelPath, ignore_errors=True)\n",
    "shutil.rmtree(tbPath, ignore_errors=True)\n",
    "os.makedirs(modelPath)\n",
    "os.makedirs(tbPath)\n",
    "tensorDir = 'processedData/allChannels/'\n",
    "targetsDir = 'processedData/allChannels/targets.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 4470)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = CustomBrainMNISTSCalogramAllChannel(eventScaloGramDir=tensorDir, targetsPath=targetsDir,type='train')\n",
    "validation_data = CustomBrainMNISTSCalogramAllChannel(eventScaloGramDir=tensorDir, targetsPath=targetsDir,type='val')\n",
    "testing_data = CustomBrainMNISTSCalogramAllChannel(eventScaloGramDir=tensorDir, targetsPath=targetsDir,type='test')\n",
    "len(training_data), len(validation_data), len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(training_args: dict):\n",
    "    global min_loss_so_far\n",
    "    batch_size = int(training_args.get('batch_size', 8))\n",
    "    device = training_args.get('dev_id')\n",
    "    lr=float(training_args.get('lr',0.0001))\n",
    "    epochs = int(training_args.get('num_epochs',10))\n",
    "    tbPath = training_args['tbPath']\n",
    "    modelPath = training_args['modelPath']\n",
    "    # modelParams = training_args['modelParams']\n",
    "    # print((batch_size, device, lr, epochs, tbPath, modelPath))\n",
    "    model_name = '_epochs=' + str(epochs) + '_lr=' + str(lr) + '_batchSize=' + str(batch_size)\n",
    "    tb = SummaryWriter(log_dir=tbPath + model_name)\n",
    "\n",
    "    training_dataloader = DataLoader(training_data, shuffle=False, batch_size=batch_size)\n",
    "    validation_dataloader = DataLoader(validation_data, batch_size=batch_size,)\n",
    "\n",
    "    model = VGG_Style_3Block(in_channels=14)\n",
    "    print(model)\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model = model.to(device)\n",
    "    train_step = 0\n",
    "    val_step = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        ## training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_train_batches = len(training_dataloader)\n",
    "        for batch, (X, y) in enumerate(training_dataloader):\n",
    "            X = X.type(torch.float)\n",
    "            train_step += 1\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_criterion(pred, y)\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "            train_loss += loss\n",
    "            tb.add_scalar('training loss vs step', loss, train_step)\n",
    "        train_loss /= num_train_batches\n",
    "\n",
    "        #validating\n",
    "        val_size = len(validation_dataloader.dataset)\n",
    "        num_val_batches = len(validation_dataloader)\n",
    "        val_loss, correct = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in validation_dataloader:\n",
    "                X = X.type(torch.float)\n",
    "                val_step += 1\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = loss_criterion(pred, y).item()\n",
    "                val_loss += loss\n",
    "                tb.add_scalar('validation loss vs step', loss, val_step)\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        val_loss /= num_val_batches\n",
    "        correct /= val_size\n",
    "        accuracy = 100*correct\n",
    "        if val_loss < min_loss_so_far:\n",
    "            min_loss_so_far = val_loss\n",
    "            checkpoint = dict()\n",
    "            checkpoint['sd'] = model.state_dict()\n",
    "            checkpoint['h_params'] = model_name\n",
    "            checkpoint['saved_epoch'] = epoch\n",
    "            torch.save(checkpoint, modelPath + 'bestModelSoFar.pt')\n",
    "        tb.add_scalar('average training loss vs epoch', train_loss, epoch)\n",
    "        tb.add_scalar('average validation loss vs epoch', val_loss, epoch)\n",
    "        tb.add_scalar('validation accuracy vs epoch', accuracy, epoch)\n",
    "    \n",
    "    del model\n",
    "    del training_dataloader\n",
    "    del validation_dataloader    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Run ID: 0 | H Params==> lr:0.001, batch_size:100, epochs:30\n",
      "VGG_Style_3Block(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Flatten(start_dim=1, end_dim=-1)\n",
      "    (10): Linear(in_features=100352, out_features=128, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fe8da0944e41d4b07f161bf65235bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 1 | H Params==> lr:0.01, batch_size:100, epochs:30\n",
      "VGG_Style_3Block(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Flatten(start_dim=1, end_dim=-1)\n",
      "    (10): Linear(in_features=100352, out_features=128, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f343e0d9544cf68f7c024ffd93fbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = dict(\n",
    "    lr = [0.001, 0.01],\n",
    "    batch_size = [100],\n",
    "    epochs = [30]\n",
    ")\n",
    "\n",
    "# modelParams = (\n",
    "#     (14,28,(45,1),1,0),\n",
    "#     (28,56,(1,64),1,0),\n",
    "#     (14,45,256)\n",
    "# )\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "## performing hyper paramete tuning\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "for run_id, (lr,batch_size, epochs) in enumerate(product(*param_values)):\n",
    "    print(\"Run ID: {} | H Params==> lr:{}, batch_size:{}, epochs:{}\".format(run_id, lr, batch_size, epochs))\n",
    "    training_args = dict()\n",
    "    training_args['num_epochs'] = epochs\n",
    "    training_args['lr'] = lr\n",
    "    training_args['batch_size'] = batch_size\n",
    "    training_args['dev_id'] = device\n",
    "    training_args['tbPath'] = tbPath\n",
    "    training_args['modelPath'] = modelPath\n",
    "    # training_args['modelParams'] = modelParams\n",
    "    model_trainer(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper params:_epochs=30_lr=0.001_batchSize=100\n",
      "Validation loss when the model was saved:2.3026712989807128\n",
      "Epoch when the model was saved:6\n",
      "Test Error: \n",
      " Accuracy: 9.3%, Avg loss: 2.302860 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(testing_data, batch_size=128)\n",
    "## perfroming test\n",
    "checkpoint = torch.load(modelPath + 'bestModelSoFar.pt')\n",
    "model = VGG_Style_3Block(in_channels=14)\n",
    "model.load_state_dict(checkpoint['sd'])\n",
    "print(\"Model hyper params:{}\".format(checkpoint['h_params']))\n",
    "print(\"Validation loss when the model was saved:{}\".format(min_loss_so_far))\n",
    "print(\"Epoch when the model was saved:{}\".format(checkpoint['saved_epoch']))\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "test_size = len(test_loader.dataset)\n",
    "num_test_batches = len(test_loader)\n",
    "test_loss, correct = 0, 0\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X = X.type(torch.float)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_criteria(pred, y).item()\n",
    "        test_loss += loss\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "test_loss /= num_test_batches\n",
    "correct /= test_size\n",
    "accuracy = 100*correct\n",
    "print(f\"Test Error: \\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anp_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
